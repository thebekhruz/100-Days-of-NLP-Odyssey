{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoi-kgJ2x1TW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from dateutil.parser import parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHSCnC7OyAuv"
      },
      "outputs": [],
      "source": [
        "NLP = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"parser\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF8Ytndjx6wI"
      },
      "outputs": [],
      "source": [
        "ABBREVIATION_EXPANSIONS = {\n",
        "    # Geographical:\n",
        "    \"PH\": \"Public house\",\n",
        "    \"R.Wye\": \"River Wye\",\n",
        "    \"HW\": \"High Wycombe\",\n",
        "    \"HQ\": \"Headquarters\",\n",
        "    \"HDC\": \"High Wycombe District Council\",\n",
        "    # Historical and Military:\n",
        "    \"W.R.A.F.\": \"Women's Royal Air Force.\",\n",
        "    \"V.E.Day\": \"Victory in Europe Day\", 'VE Day': \"Victory in Europe Day\",\n",
        "    \"WWI\": \"First World War\",\n",
        "    \"WWII\": \"Second World War\",\n",
        "    \"RAF\": \"Royal Air Force\",\n",
        "    \"HMS\": \"Her Majesty's Ship\",\n",
        "    \"HRH\": \"His Royal Highness\",\n",
        "    \"MP\": \"Member of Parliament\",\n",
        "    \"BFP\": \"Bucks Free Press\",\n",
        "    \"VJ Day\": \"Victory over Japan Day\", \"VJ-day\": \"Victory over Japan Day\",\n",
        "    # Compass directions\n",
        "    \"N\": \"North\",\n",
        "    \"S\": \"South\",\n",
        "    \"E\": \"East\",\n",
        "    \"W\": \"West\",\n",
        "    \"NE\": \"Northeast\",\n",
        "    \"NW\": \"Northwest\",\n",
        "    \"SE\": \"Southeast\",\n",
        "    \"SW\": \"Southwest\",\n",
        "    \"NNE\": \"North-Northeast\",\n",
        "    \"ENE\": \"East-Northeast\",\n",
        "    \"ESE\": \"East-Southeast\",\n",
        "    \"SSE\": \"South-Southeast\",\n",
        "    \"SSW\": \"South-Southwest\",\n",
        "    \"WSW\": \"West-Southwest\",\n",
        "    \"WNW\": \"West-Northwest\",\n",
        "    \"NNW\": \"North-Northwest\",\n",
        "    # Street Names\n",
        "    \"St\": \"Street\",\n",
        "    \"Rd\": \"Road\",\n",
        "    \"Ave\": \"Avenue\",\n",
        "    \"Dr\": \"Drive\",\n",
        "    \"Ln\": \"Lane\",\n",
        "    \"Ct\": \"Court\",\n",
        "    \"Sq\": \"Square\",\n",
        "    # Dates\n",
        "    \"c.\": \"circa\", \"c\" : \"circa\",\n",
        "    \"Mon.\": \"Monday \", \"Mon\": \"Monday\",\n",
        "    \"Tue.\": \"Tuesday \", \"Tue\": \"Tuesday\",\n",
        "    \"Wed.\": \"Wednesday \", \"Wed\": \"Wednesday\",\n",
        "    \"Thu.\": \"Thursday \", \"Thu\": \"Thursday\",\n",
        "    \"Fri.\": \"Friday \", \"Fri\": \"Friday\",\n",
        "    \"Sat.\": \"Saturday \", \"Sat\": \"Saturday\",\n",
        "    \"Sun.\": \"Sunday \", \"Sun\": \"Sunday\",\n",
        "    \"Jan.\": \"January \", \"Jan\": \"January \",\n",
        "    \"Feb.\": \"February \", \"Feb\": \"February\",\n",
        "    \"Mar.\": \"March \", \"Mar\": \"March\",\n",
        "    \"Apr.\": \"April \", \"Apr\": \"April\",\n",
        "    \"May\": \"May\",\n",
        "    \"Jun.\": \"June \", \"Jun\": \"June\",\n",
        "    \"Jul.\": \"July \", \"Jul\": \"July\",\n",
        "    \"Aug.\": \"August \", \"Aug\": \"August\",\n",
        "    \"Sep.\": \"September \", \"Sep\": \"September \", \"Sept.\": \"September \", \"Sept\": \"September \",\n",
        "    \"Oct.\": \"October \", \"Oct\": \"October\",\n",
        "    \"Nov.\": \"November \", \"Nov\": \"November\",\n",
        "    \"Dec.\": \"December \", \"Dec\": \"December\",\n",
        "    # Other\n",
        "    \"FC\": \"Football Club\"\n",
        "\n",
        "    # Extend this list as needed\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMjb4QJk1ow-"
      },
      "outputs": [],
      "source": [
        "APOSTROPHE_PATTERN = re.compile(r\"[^\\w\\d\\s]+\")\n",
        "BRACKET_PATTERN = re.compile(r\"\\[|\\]\")\n",
        "NUMBER_WITH_S_PATTERN = re.compile(r\"\\b(\\d+)s\\b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcwq8cfsSB06",
        "outputId": "e551ee69-dedf-432d-9061-9f6cdab79b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Success! \n",
            "Execution Time: 156.68444299697876 seconds\n"
          ]
        }
      ],
      "source": [
        "def expand_abbreviations(text):\n",
        "    \"\"\"Replace abbreviations in the text with their expanded forms.\"\"\"\n",
        "    for abbr, expansion in ABBREVIATION_EXPANSIONS.items():\n",
        "        pattern = r'\\b' + re.escape(abbr) + r'\\b(?![\\'â€™])'\n",
        "        text = re.sub(pattern, expansion, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean a single text string by removing apostrophes, brackets, and trailing 's' in numbers.\"\"\"\n",
        "    text = APOSTROPHE_PATTERN.sub(\"\", text)\n",
        "    text = NUMBER_WITH_S_PATTERN.sub(r\"\\1\", text)  # Replace with the captured number, removing the 's'\n",
        "    text = BRACKET_PATTERN.sub(\" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_texts_in_batches(texts, batch_size=256*8):\n",
        "    \"\"\"Process texts in batches with preprocessing steps.\"\"\"\n",
        "    preprocessed_texts = [\n",
        "        expand_abbreviations(clean_text(text if isinstance(text, str) else \"[ No Description Available ]\"))\n",
        "        for text in texts\n",
        "    ]\n",
        "\n",
        "    processed_texts = []\n",
        "    for doc in NLP.pipe(preprocessed_texts, batch_size=batch_size):\n",
        "        processed_text = \" \".join(token.text.lower() for token in doc if not token.is_stop and not token.is_punct)\n",
        "        processed_texts.append(processed_text)\n",
        "\n",
        "    return processed_texts\n",
        "\n",
        "\n",
        "def get_texts_with_ids(df, text_type):\n",
        "    \"\"\"Get texts and their doc_ids based on type (title/description).\"\"\"\n",
        "    relevant_df = df[df['type'] == text_type]\n",
        "    return relevant_df['doc_id'].tolist(), relevant_df['value'].tolist()\n",
        "\n",
        "\n",
        "def save_processed_data(doc_ids, processed_texts, output_file_path, types):\n",
        "    \"\"\"Save processed texts with their doc_ids to a CSV file.\"\"\"\n",
        "    processed_df = pd.DataFrame({'doc_id': doc_ids, 'type': types, 'value': processed_texts})\n",
        "    processed_df.to_csv(output_file_path, sep='\\t', index=False, header=None)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate the preprocessing pipeline.\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Define file paths\n",
        "    input_file_path = '/content/swop_triples_cleaned.csv'\n",
        "    output_file_path_titles = '/content/processed_data_title_no_lemma.csv'\n",
        "    output_file_path_descriptions = '/content/processed_data_descr_no_lemma.csv'\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(input_file_path, delimiter='\\t', header=None, names=['doc_id', 'type', 'value'])\n",
        "\n",
        "    # Process and save titles\n",
        "    title_ids, titles = get_texts_with_ids(df, 'title')\n",
        "    processed_titles = preprocess_texts_in_batches(titles)\n",
        "    save_processed_data(title_ids, processed_titles, output_file_path_titles, 'title')\n",
        "\n",
        "    # Process and save descriptions\n",
        "    description_ids, descriptions = get_texts_with_ids(df, 'description')\n",
        "    processed_descriptions = preprocess_texts_in_batches(descriptions)\n",
        "    save_processed_data(description_ids, processed_descriptions, output_file_path_descriptions, 'description')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nSuccess! \\nExecution Time: {end_time - start_time} seconds\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $Score = Semantic Similarity + λ × Contextual Relevance$\n",
    "\n",
    "## Goal:\n",
    "* This Notebook will try to calculate the $Contextual Relevance$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the small English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = '/Users/thebekhruz/Desktop/100Days-Of-Code/100-Days-of-NLP-Odyssey/data/raw/data_formatted_date.jsonl'\n",
    "df = pd.read_json(input_file_path, lines=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete relations column as it does not have any meaningful information\n",
    "del df['relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and remove rows with empty 'mentions' dictionaries\n",
    "if df['mentions'].isnull().sum()>0:\n",
    "    df = df.dropna(subset=['mentions'])\n",
    "    print('NaN mentions rows where removed.')\n",
    "else:\n",
    "    print('There are no empty mentions dictionaries in the database')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific keys from dictionaries in the 'mentions' column and create a new column\n",
    "def extract_key_from_dict_list(dict_list, key):\n",
    "    if isinstance(dict_list, list):\n",
    "        result = [element.get(key) for element in dict_list if isinstance(element, dict) and key in element]\n",
    "        return result\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function applies the extraction of a key for a series in a DataFrame\n",
    "def apply_extraction_to_column(df, column_name, key, new_column_name):\n",
    "    df[new_column_name] = df[column_name].apply(lambda x: extract_key_from_dict_list(x, key))\n",
    "    return df\n",
    "\n",
    "\n",
    "df = apply_extraction_to_column(df, 'mentions', 'ne_span', 'extracted_entities')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesses the given text by removing punctuation, making text lowercase, and removing stop words.\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- * If you want to calculate the TF-IDF scores for all the words in your preprocessed text, then you should use preprocessed_text.\n",
    "* If you are specifically interested in the TF-IDF scores for the named entities (i.e., ne_span) only, then you should use preprocessed_entities. -->\n",
    "\n",
    "$ Score = Semantic Similarity   * w * Contextual Relevance (TF-IDF) $\n",
    "#### Significance of Named Entities Within Documents:\n",
    "* To prioritize named entities in documents, focus on *preprocessed_entities* for TF-IDF calculations. This emphasizes entity importance independently of surrounding text.\n",
    "#### Semantic Similarity and Contextual Relevance:\n",
    "* TF-IDF fine-tunes the $Score$ by giving more importance to specific named entities. Using *preprocessed_entities* provides a focused relevance score on the entities without diluting the effect sorounding text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocess_text function on the text and exctracted_entities column\n",
    "# Uncomment this if you think that contextual information is important\n",
    "    # df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "df['preprocessed_entities'] = df['extracted_entities'].apply(lambda x: [preprocess_text(entity) for entity in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all arrays to create a single list of preprocessed entities across all documents\n",
    "all_entities = sum(df['preprocessed_entities'], [])\n",
    "\n",
    "# Convert this list into a string where each entity is separated by a space (to simulate a \"document\" of entities)\n",
    "entities_text = ' '.join(all_entities)\n",
    "\n",
    "# Create a \"document\" for each set of entities in each row to calculate TF-IDF scores\n",
    "entities_documents = [' '.join(entities) for entities in df['preprocessed_entities']]\n",
    "\n",
    "# Initialize the vectorizer\n",
    "entity_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the entities documents to calculate TF-IDF\n",
    "entity_tfidf_matrix = entity_vectorizer.fit_transform(entities_documents)\n",
    "\n",
    "# tfidf_df = pd.DataFrame(entity_tfidf_matrix.toarray(), columns=entity_vectorizer.get_feature_names_out())\n",
    "tfidf_df = pd.DataFrame(entity_tfidf_matrix.toarray(), index=df['IAID'], columns=entity_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the shape of the TF-IDF matrix\n",
    "print(f'The shape of the TF-IDF matrix is: {entity_tfidf_matrix.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the TF-IDF score for a word in a specific document using IAID\n",
    "def get_tfidf_score(word, vectorizer, tfidf_df, iaid):\n",
    "    index = vectorizer.vocabulary_.get(word)\n",
    "    # If the word is in the vocabulary, return its score for the specific document based on IAID\n",
    "    if index is not None:\n",
    "        return tfidf_df.loc[iaid, vectorizer.get_feature_names_out()[index]]\n",
    "    else:\n",
    "        # If the word is not in the vocabulary, return 0\n",
    "        return 0\n",
    "\n",
    "# Function to calculate the total TF-IDF score for each mention's ne_span for a specific document using IAID\n",
    "def add_tfidf_scores_to_mentions(row, vectorizer, tfidf_df):\n",
    "    mentions = row['mentions']\n",
    "    iaid = row['IAID']  # Use IAID to reference the document in tfidf_df\n",
    "    for mention in mentions:\n",
    "        words = mention['ne_span'].lower().split()\n",
    "        # Note: Ensure that `preprocess_text` is applied here if necessary, as per your preprocessing logic\n",
    "        total_score = sum(get_tfidf_score(word, vectorizer, tfidf_df, iaid) for word in words)\n",
    "        mention['total_tfidf_score'] = total_score\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "df.apply(lambda row: add_tfidf_scores_to_mentions(row, entity_vectorizer, tfidf_df), axis=1)\n",
    "df['mentions'][0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns that are no longer needed to descrease the data size\n",
    "# del df['preprocessed_text'] # If you have used it. Uncomment it\n",
    "del df['preprocessed_entities']\n",
    "del df['extracted_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new data as 'data_with_contextual_relevance.jsonl'\n",
    "output_file_path = '/Users/thebekhruz/Desktop/100Days-Of-Code/100-Days-of-NLP-Odyssey/data/processed/data_with_contextual_relevance.jsonl'\n",
    "df.to_json(output_file_path, orient='records', lines=True)\n",
    "print(f'Data saved to {output_file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
